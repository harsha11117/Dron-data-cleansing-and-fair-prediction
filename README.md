#### Data cleaning is the process of ensuring that your data is correct, consistent and useable.

The first step of the cleansing process is data auditing. In this step, we identify the types of anomalies that reduce the data quality.  Data auditing is about programmatically checking the data using some validation rules that are pre-specified, and then creating a report of the quality of the data and its problems. We often apply some statistical tests in this step for examining the data.
Data Anomalies can be classified at a high level into three categories:

1. **Syntactic Anomalies**: 

2. **Semantic Anomalies**: 

3. **Coverage Anomalies**:

packages used for this project are:

![alt text](https://github.com/harsha111117/Drone-data-cleansing-and-fair-prediction/blob/master/packages.jpg?raw=true

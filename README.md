#### Data cleaning is the process of ensuring that your data is correct, consistent and useable.

The first step of the cleansing process is data auditing. In this step, we identify the types of anomalies that reduce the data quality.  Data auditing is about programmatically checking the data using some validation rules that are pre-specified, and then creating a report of the quality of the data and its problems. We often apply some statistical tests in this step for examining the data.
Data Anomalies can be classified at a high level into three categories:

1. **Syntactic Anomalies**: 

2. **Semantic Anomalies**: 

3. **Coverage Anomalies**:

packages used for this project are:

![alt Packages](https://user-images.githubusercontent.com/57734773/89722688-f069b900-da2f-11ea-9754-4f151492b883.jpg?raw=true)


This data is about the drones deliverying the packages across the victoria. Its has 16 columns and 37503 rows. Some of the columns have missing values that can be found by looking at the output from the .info() command on dataframe.


![datatype_nulls](https://user-images.githubusercontent.com/57734773/89722786-222f4f80-da31-11ea-800b-ee21e3dadd28.jpg)


